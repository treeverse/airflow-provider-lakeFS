# Mixing Metadata, Air and Water: Use the lakeFS Airflow Provider to Link Airflow Execution to lakeFS Data

By [Ariel Shaqed (Scolnicov)]

## Introduction

"How do I integrate X with lakeFS" is an ever-green question on [lakeFS
Slack][lakefs-slack].  lakeFS takes a "tooling-first" strategy to data
management: it slots into your existing lineup of tools.  So a significant
part of our work on lakeFS is devoted to leveraging the tools mutually to
get improve these integrations.  Our latest addition is Project Syphon!

<img alt="Soda syphon, by Avi Nahmias, available from https://commons.wikimedia.org/wiki/User:Avin" src="https://upload.wikimedia.org/wikipedia/commons/1/1c/Soda_syphon_IMG_1505C.jpg" width=400/>

Airflow was one of our earliest integrations: Itai published the original
[Air and Water][airflow-air-and-water-blog-1] 2 years ago.  It's time to
deepen our integrations!  The latest release of the lakeFS provider for
Airflow, codename "Project Syphon", adds deep linking between Airflow DAG
runs and the lakeFS commits that they create.  It connects the two for
interactive or programmatic use.

The new Airflow operators add formatted metadata to your commits.  lakeFS
already uses this metadata to link to Airflow.  These metadata keys also
work with any formatted metadata keys.  You can leverage such metadata to
add your own operable metadata on any commits you perform from _any_ tools
(not only from Airflow).

How are you integrating these new features into your workflows?  How can we
improve them and help you?  Please let us know on [lakeFS
Slack][lakefs-slack] or by [opening an issue][airflow-provider-new-issue]!

## Using the lakeFS Airflow provider

Links are generated by lakeFS provider versions 0.46.1 and above.  To use
them, ensure your Airflow `requirements.txt` includes

```py
airflow-provider-lakefs>=0.46.1
```

That is pretty much all.  Using LakeFSCommitOperator or LakeFSMergeOperator
normally in your DAGs will add metadata to your commits.

lakeFS can interpret Airflow provider metadata to link back to Airflow
starting at version **TBD TBD** 0.100.0 **TBD TBD**.

### Metadata on lakeFS commits

When examining a commit in lakeFS, the UI shows metadata and a link "Open
Airflow UI" to the DAG run in the UI.

![lakeFS commit UI with "Open Airflow UI" button][lakefs-commit-ui-open-button]

The button "Open Airflow UI" links to the generating DAG run on Airflow.
Metadata from the DAG run on Airflow is added to the commit, in keys
starting "::lakefs::".

### Link on Airflow

When examining the task of a LakeFSCommitOperator or a LakeFSMergeOperator
of a workflow on Airflow, the UI shows a "lakeFS" button.

![Airflow DAG UI with "lakeFS" button][airflow-task-ui-open-button]

This button links to the generated commit on lakeFS.

## How it works

Knowing how all this works can help you perform additional tasks by
leveraging these features.  Part of the tooling approach of Airflow and
lakeFS is to provide simple interfaces that can readily be used to integrate
and reliably extract data.  Syphon is built by combining tools:

* Extra links in Airflow tasks
* Commit metadata in lakeFS
* A new convention for creating link buttons from commit metadata

By leveraging existing features in lakeFS and in Airflow, and adding a small
useful convention, we ensure that _other_ tools can also leverage these
features.

### lakeFS structured metadata

When the lakeFS Airflow provider creates a commit, it creates metadata from
the DAG run.  All metadata uses the key prefix `::lakefs::Airflow::`
prefixes.  We _ask_ that you create metadata with key prefix `::lakefs::` in
accordance with specific guidelines.  Doing so will allow metadata to
continue to be useful as we add more lakeFS-side features to commit
metadata.

Fields added by the Airflow provider mostly parallel the fields documented
in the [DagRun REST API][airflow-dagrun-rest-api].  We use structured
metadata keys, ensuring that keys from different products can be separated
and correctly handled automatically in most circumstances.

Structured metadata keys have the format "`::lakefs::LABEL::KEY[TYPE]`":

* `LABEL` should be a human-readable name of the generating process or
  product.  The Airflow provider uses `Airflow`.
* `KEY` should describe the value.
* `[TYPE]` should describe the type of the value.  Currently we define several types:
  * To specify a plain string, just use "`::lakefs::LABEL::KEY`".

    The Airflow provider places multiple string fields from the DAG run,
    including "`::lakefs::Airflow::dag_id`",
    "`::lakefs::Airflow::dag_run_id`", "`::lakefs::Airflow::run_type`", and
    "`::lakefs::Airflow::note`".
  * `[url:ui]` specifies that the value contains a URL for a UI.  The lakeFS
    UI identifies such metadata and creates a clickable button.

    The Airflow provider places the DAG run UI URL in
    "`::lakefs::Airflow::url[url:ui]`".  This allows the lakeFS UI to
    generate the "Open Airflow UI" button.
  * `[url:id]` specifies that the value contains an identifying URL.  This
    can be used by other clients to access the generating resource.

    The Airflow provider places the URL of the DAG run in the Airflow API in
    "`::lakefs::Airflow::url[url:id]`".  Clients that need to access the
    Airflow DAG run can use this URL as their base.
  * `[iso8601]` specifies that the value contains a timestamp in ISO 8601
    format.

    The Airflow provider places all its timestamps in such keys, including
	"`::lakefs::Airflow::logical_date[iso8601]`",
	"`::lakefs::Airflow::data_interval_start[iso8601]`",
	"`::lakefs::Airflow::data_interval_end[iso8601]`", and
	"`::lakefs::Airflow::last_scheduling_decision[iso8601]`".
  * `[boolean]` specifies a boolean encoded as a string `true` or `false`.

### Bonus!  Airflow extra links

The Airflow provider creates the "lakeFS" link from a task to the commit
that it generates.  This is just an [extra link][airflow-extra-links].
Generating the link requires additional data.  This data is stored on XCom
using the key `lakefs_commit`.  That key is a Python `dict`:

```py
{
  'base_url': 'https://example-org.us-east-1.lakefscloud.io',
  'repo': 'example-repo',
  'commit_digest': '51ee91ec0ffee...'
}
```

[airflow-air-and-water-blog-1]:  https://lakefs.io/blog/the-airflow-and-lakefs-integration/
[airflow-provider-new-issue]:  https://github.com/treeverse/airflow-provider-lakeFS/issues/new
[lakefs-commit-ui-open-button]:  Screenshot-lakefs-to-airflow-link.png
[airflow-task-ui-open-button]:  Screenshot-airflow-to-lakefs-link.png
[lakefs-slack]:  https://lakefs.io/slack
[airflow-dagrun-rest-api]:  https://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html#tag/DAGRun
