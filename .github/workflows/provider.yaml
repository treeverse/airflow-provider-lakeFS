name: Provider
on:
  pull_request:
  push:
    branches:
      - main

jobs:
  build-provider:
    name: build latest provider
    runs-on: ubuntu-latest
    env:
      KEY: "AKIAIOSFODNN7EXAMPLE"
      SECRET: "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
    steps:
      - name: Checkout
        uses: actions/checkout@v2

      - name: Install requirements
        run: pip install -r requirements.txt

      - name: Build package
        run: |
          python3 -m pip install build
          python3 -m build

      - name: Install astro
        env:
          TAG: 0.28.1
        run: curl -sSL https://install.astronomer.io | sudo bash -s

      - name: Create astro dev env
        run: mkdir astro && cd astro && astro dev init

      - name: Copy files
        run: |
          cp -R ./dist/ astro/
          cp ./lakefs_provider/example_dags/lakefs-dag.py astro/dags/

      - name: Insert Dockerfile pip install
        run: printf "\nRUN pip install --user dist/airflow_provider_lakefs-*-py3-none-any.whl" >> astro/Dockerfile

      - name: Start astro
        run: |
          cd astro && astro dev start

      - name: spin up lakeFS
        run: docker-compose -f ops/docker-compose.yaml up --quiet-pull -d && sleep 30

      - name: Setup lakeFS
        run: |
          curl localhost:8000/api/v1/setup_lakefs -H "Content-Type: application/json" --request POST --data '{"username":"test","key":{"access_key_id":"${{ env.KEY }}","secret_access_key":"${{ env.SECRET }}"}}'

      - name: Create test repo
        run: |
         export BASIC_AUTH=$(echo -n "${{ env.KEY }}:${{ env.SECRET }}" | base64)
         curl localhost:8000/api/v1/repositories -H "Content-Type: application/json" -H "Authorization: Basic $(echo $BASIC_AUTH | tr -d ' ')" --request POST --data '{"name":"example-repo","storage_namespace":"local://data/"}'

      - name: Run lakeFS DAG
        run: |
          cd astro
          astro dev run connections add conn_lakefs --conn-type=HTTP --conn-host=http://172.17.0.1:8000 --conn-login="${{ env.KEY }}" --conn-password="${{ env.SECRET }}"
          astro dev run dags unpause lakeFS_workflow
          astro dev run dags trigger lakeFS_workflow
          sleep 30

      - name : Run DAG state check script
        id : dag_status_id
        run: |
            chmod +x dag_status.py
            python3 dag_status.py

      - name: Wait until Airflow makes output file available on main
        env:
          # To avoid the lack of region - see https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-envvars.html
          AWS_EC2_METADATA_DISABLED: true
        uses: nick-invision/retry@v2
        with:
          timeout_minutes: 3
          max_attempts: 30
          command: AWS_ACCESS_KEY_ID=${{ env.KEY }} AWS_SECRET_ACCESS_KEY=${{ env.SECRET }} aws s3 cp --endpoint-url=http://s3.local.lakefs.io:8000 s3://example-repo/main/path/to/_SUCCESS -

      - name: Wait symlink file creation and validate it
        env:
          AWS_EC2_METADATA_DISABLED: true
        uses: nick-invision/retry@v2
        with:
          timeout_minutes: 3
          max_attempts: 30
          command: docker-compose -f ops/docker-compose.yaml exec -T lakefs ls lakefs/data/block/data/symlinks/example-repo/example-branch/path/to/symlink.txt

      - name: lakeFS logs
        if: ${{ always() }}
        run: docker-compose -f ops/docker-compose.yaml logs --tail=1000 lakefs

      - name: airflow scheduler logs
        if: ${{ always() }}
        run: |
          cd astro
          astro dev logs --scheduler

      - name: airflow triggerer logs
        if: ${{ always() }}
        run: |
          cd astro
          astro dev logs --triggerer
